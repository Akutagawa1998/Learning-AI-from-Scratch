# Learning AI from Scratch

ä»é›¶å¼€å§‹å­¦ä¹ äººå·¥æ™ºèƒ½å’Œæ·±åº¦å­¦ä¹ çš„é¡¹ç›®ã€‚æœ¬é¡¹ç›®é‡‡ç”¨**12å‘¨ï¼ˆ84å¤©ï¼‰æ¯æ—¥è®¡åˆ’**ï¼Œæ¯å¤©2å°æ—¶ï¼Œç³»ç»Ÿæ€§åœ°ä»åŸºç¡€è®­ç»ƒè„šæ‰‹æ¶åˆ°å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œæœ€ç»ˆå®Œæˆå¤§æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–çš„å®Œæ•´æµç¨‹ã€‚

## ğŸ“‹ å­¦ä¹ è®¡åˆ’æ¦‚è§ˆ

æœ¬é¡¹ç›®éµå¾ª**12å‘¨ï¼ˆ84å¤©ï¼‰æ¯æ—¥è®¡åˆ’**ï¼Œæ¯å¤©å›ºå®š2å°æ—¶ï¼Œåˆ†ä¸ºï¼š
- **20åˆ†é’Ÿ**ï¼šè¾“å…¥å­¦ä¹ 
- **80åˆ†é’Ÿ**ï¼šå·¥ç¨‹/å®éªŒ
- **20åˆ†é’Ÿ**ï¼šè®°å½•/å¤ç›˜

### 12å‘¨è·¯çº¿å›¾

| å‘¨æ¬¡ | ä¸»é¢˜ | æ ¸å¿ƒç›®æ ‡ |
|------|------|----------|
| **Week 1** | è®­ç»ƒè„šæ‰‹æ¶ v0 | å•å¡è·‘é€šï¼Œå¯å¤ç°è®­ç»ƒé—­ç¯ï¼ˆMNISTï¼‰ |
| **Week 2** | å¤šå¡ DDP + AMP | 4å¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œåå/æ˜¾å­˜ profiling |
| **Week 3** | è®­ç»ƒå­¦æ ¸å¿ƒ | CIFAR10 + MLP/CNNï¼Œè§„èŒƒ ablation |
| **Week 4** | Transformer ç»„ä»¶ | å®ç° decoder-only blockï¼Œå­—ç¬¦çº§ LM |
| **Week 5** | LM Pipeline | Tokenizer + pack + ppl eval + benchmark |
| **Week 6** | å¤šå¡å®éªŒè§„èŒƒ | åŒé¢„ç®—å¯¹ç…§ï¼Œç ”ç©¶å¼å†³ç­–èƒ½åŠ› |
| **Week 7** | SFT é—­ç¯ | æŒ‡ä»¤å¾®è°ƒ + è¯„æµ‹ harness |
| **Week 8** | LoRA / QLoRA | PEFT å¯¹ç…§ä¸å†³ç­– |
| **Week 9** | DPO | åå¥½ä¼˜åŒ–è®­ç»ƒä¸è¯„æµ‹ |
| **Week 10** | åˆ†å¸ƒå¼æ˜¾å­˜æ–¹æ¡ˆ | FSDP/ZeRO ä½¿ç”¨ä¸æ€§èƒ½åˆ†æ |
| **Week 11** | æ¨ç†ä¼˜åŒ– | KV cache + FlashAttention |
| **Week 12** | ä½œå“é›†æ‰“åŒ… | é¢è¯•èµ„äº§æ•´ç† |

è¯¦ç»†æ¯æ—¥è®¡åˆ’è¯·å‚è€ƒ [Plan.md](Plan.md)

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
Learning-AI-from-Scratch/
â”œâ”€â”€ my-dl-plan/              # æ·±åº¦å­¦ä¹ è®¡åˆ’ç›®å½•
â”‚   â”œâ”€â”€ configs/             # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”‚   â””â”€â”€ baseline.yaml    # åŸºçº¿é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ src/                 # æºä»£ç ç›®å½•
<<<<<<< Updated upstream
â”‚   â”‚   â”œâ”€â”€ data/            # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”‚   â”‚   â””â”€â”€ mnist.py     # MNIST æ•°æ®åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ models/          # æ¨¡å‹å®šä¹‰ï¼ˆå¾…å®Œå–„ï¼‰
â”‚   â”‚   â”œâ”€â”€ train/           # è®­ç»ƒæ¨¡å—ï¼ˆå¾…å®Œå–„ï¼‰
â”‚   â”‚   â”œâ”€â”€ eval/            # è¯„ä¼°æ¨¡å—ï¼ˆå¾…å®Œå–„ï¼‰
â”‚   â”‚   â””â”€â”€ utils/           # å·¥å…·å‡½æ•°ï¼ˆå¾…å®Œå–„ï¼‰
â”‚   â”œâ”€â”€ scripts/             # è„šæœ¬ç›®å½•
â”‚   â”‚   â””â”€â”€ slurm/           # Slurm æäº¤è„šæœ¬ï¼ˆå¾…å®Œå–„ï¼‰
â”‚   â”œâ”€â”€ results/             # ç»“æœç›®å½•
â”‚   â”‚   â””â”€â”€ week01/          # æ¯å‘¨ç»“æœ
â”‚   â”‚       â”œâ”€â”€ metrics.csv  # æŒ‡æ ‡è®°å½•
â”‚   â”‚       â”œâ”€â”€ plots/       # å›¾è¡¨
â”‚   â”‚       â””â”€â”€ notes.md     # å®éªŒç¬”è®°
=======
â”‚   â”‚   â”œâ”€â”€ models/          # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â””â”€â”€ data/            # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”‚       â””â”€â”€ mnist.py     # MNIST æ•°æ®åŠ è½½å™¨
>>>>>>> Stashed changes
â”‚   â”œâ”€â”€ notes/               # ç¬”è®°ç›®å½•
â”‚   â”‚   â”œâ”€â”€ env.md           # ç¯å¢ƒé…ç½®ç¬”è®°
â”‚   â”‚   â””â”€â”€ day3.md          # Day 3 è®­ç»ƒç¬”è®°
â”‚   â”œâ”€â”€ requirements.txt     # Python ä¾èµ–åŒ…
â”‚   â”œâ”€â”€ train.py             # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ REPRODUCE.md         # å¤ç°æŒ‡å—ï¼ˆå¾…å®Œå–„ï¼‰
â”œâ”€â”€ Plan.md                  # å­¦ä¹ è®¡åˆ’
â””â”€â”€ README.md                # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## âš™ï¸ ç¯å¢ƒè¦æ±‚

- **Python**: 3.8+
- **PyTorch**: 2.9.1
- **CUDA**: å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ
- **é›†ç¾¤ç¯å¢ƒ**: æ”¯æŒ Slurmï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä½¿ç”¨ tmux/nohupï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å…‹éš†é¡¹ç›®

```bash
git clone <repository-url>
cd Learning-AI-from-Scratch
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

ä½¿ç”¨ condaï¼ˆæ¨èï¼‰ï¼š

```bash
conda create -n dl python=3.10
conda activate dl
```

æˆ–ä½¿ç”¨ venvï¼š

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate  # Windows
```

### 3. å®‰è£…ä¾èµ–

```bash
cd my-dl-plan
# Apple (macOS / Apple Silicon)
pip install -r requirements-apple.txt

# CUDA (Linux + NVIDIA GPU)
pip install -r requirements-cuda.txt
```

### 4. è¿è¡Œè®­ç»ƒ

#### å•å¡è®­ç»ƒï¼ˆå½“å‰é˜¶æ®µï¼‰

```bash
cd my-dl-plan
python train.py --config configs/baseline.yaml
```

<<<<<<< Updated upstream
#### å¤šå¡è®­ç»ƒï¼ˆWeek 2+ï¼‰

ä½¿ç”¨ Slurm æäº¤ï¼ˆç¤ºä¾‹ï¼‰ï¼š

```bash
sbatch scripts/slurm/mnist_ddp.sh
```

æˆ–ç›´æ¥ä½¿ç”¨ torchrunï¼š

```bash
torchrun --nproc_per_node=4 train.py --config configs/mnist_ddp.yaml
```

## ğŸ“ é…ç½®æ–‡ä»¶è¯´æ˜
=======
### Day 3 çº¿æ€§æ¨¡å‹ï¼ˆSoftmax Regressionï¼‰

```bash
cd my-dl-plan
python train.py --config configs/linear_mnist.yaml
```

### é…ç½®æ–‡ä»¶è¯´æ˜
>>>>>>> Stashed changes

é…ç½®æ–‡ä»¶é‡‡ç”¨ YAML æ ¼å¼ï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š

### ç¤ºä¾‹é…ç½®

```yaml
run:
  name: "baseline"
  seed: 42
  results_dir: "results"

train:
  epochs: 1
  batch_size: 64
  lr: 0.001

system:
  device: "auto"   # auto | cpu | cuda
  num_workers: 2
```

### é…ç½®é¡¹è¯´æ˜

- **run**: è¿è¡Œé…ç½®
  - `name`: è¿è¡Œåç§°
  - `seed`: éšæœºç§å­ï¼ˆç”¨äºå¯å¤ç°æ€§ï¼‰
  - `results_dir`: ç»“æœä¿å­˜ç›®å½•

- **train**: è®­ç»ƒé…ç½®
  - `epochs`: è®­ç»ƒè½®æ•°
  - `batch_size`: æ‰¹æ¬¡å¤§å°
  - `lr`: å­¦ä¹ ç‡

- **system**: ç³»ç»Ÿé…ç½®
  - `device`: è®¾å¤‡é€‰æ‹©ï¼ˆ`auto`/`cpu`/`cuda`ï¼‰
  - `num_workers`: æ•°æ®åŠ è½½çš„ worker æ•°é‡

## ğŸ“Š å½“å‰è¿›åº¦

### Week 1: è®­ç»ƒè„šæ‰‹æ¶ v0

**å·²å®Œæˆ**ï¼š
- âœ… Day 1: ç¯å¢ƒä¸éª¨æ¶ï¼ˆrepo åˆå§‹åŒ–ã€åŸºç¡€ç¯å¢ƒï¼‰
- âœ… Day 2: æ•°æ®ç®¡çº¿ï¼ˆMNIST æ•°æ®åŠ è½½å™¨ï¼‰
- ğŸ”„ Day 3-7: è¿›è¡Œä¸­...

**å¾…å®Œæˆ**ï¼š
- [ ] Day 3: æœ€å°æ¨¡å‹ï¼ˆSoftmax Regressionï¼‰
- [ ] Day 4: è®­ç»ƒå¾ªç¯ï¼ˆckpt + evalï¼‰
- [ ] Day 5: å¤ç°æ€§ï¼ˆseed ä¸é…ç½®ï¼‰
- [ ] Day 6: æœ€å°å¯¹ç…§ï¼ˆå­¦ä¹ ç‡ï¼‰
- [ ] Day 7: å‘¨æ¸…ç†ä¸ç‰ˆæœ¬æ ‡è®°

## ğŸ¯ æ¯å‘¨äº¤ä»˜ç‰©

æ¯å‘¨å¿…é¡»äº§å‡ºï¼š

1. **REPRODUCE.md** æ›´æ–°ï¼ˆèƒ½ä»é›¶å¤ç°ï¼‰
2. **results/weekXX/metrics.csv + plots/**ï¼ˆæŒ‡æ ‡ä¸å›¾è¡¨ï¼‰
3. **results/weekXX/notes.md**ï¼ˆå‡è®¾-æ–¹æ³•-ç»“æœ-è§£é‡Š-ä¸‹ä¸€æ­¥ï¼‰

## ğŸ“ˆ è¾“å‡ºè¯´æ˜

è®­ç»ƒè¿è¡Œåï¼Œä¼šåœ¨ `results_dir` ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„ç»“æœç›®å½•ï¼š

```
results/
â””â”€â”€ baseline_20251220-143022/
    â”œâ”€â”€ checkpoints/         # æ¨¡å‹æ£€æŸ¥ç‚¹
    â”œâ”€â”€ metrics.csv          # è®­ç»ƒæŒ‡æ ‡
    â””â”€â”€ logs/                # è®­ç»ƒæ—¥å¿—
```

## ğŸ”§ é›†ç¾¤å·¥ä½œæµï¼ˆWeek 2+ï¼‰

### Slurm æäº¤è„šæœ¬æ¨¡æ¿

```bash
#!/bin/bash
#SBATCH -J mnist_ddp
#SBATCH -p a100
#SBATCH -N 1
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=16
#SBATCH --mem=64G
#SBATCH -t 02:00:00
#SBATCH -o logs/%x-%j.out

set -euo pipefail
source ~/.bashrc
conda activate dl

export NCCL_DEBUG=WARN
export TORCH_DISTRIBUTED_DEBUG=DETAIL
export OMP_NUM_THREADS=8

<<<<<<< Updated upstream
python -m torch.distributed.run --nproc_per_node=4 \
  train.py --config configs/mnist_ddp.yaml
```

## ğŸ“š å­¦ä¹ åŸåˆ™

1. **æ¯å¤©2å°æ—¶å›ºå®šåˆ‡åˆ†**ï¼š20åˆ†é’Ÿå­¦ä¹  + 80åˆ†é’Ÿå·¥ç¨‹ + 20åˆ†é’Ÿå¤ç›˜
2. **æ¯å¤©å¿…é¡»äº§å‡ºå¯éªŒè¯ç»“æœ**ï¼šè®­ç»ƒè„šæœ¬å¯è¿è¡Œ / æäº¤æˆåŠŸ / æŒ‡æ ‡æ›²çº¿ç”Ÿæˆ
3. **æ¯å‘¨å›ºå®šä¸‰ä»¶å¥—äº¤ä»˜**ï¼šREPRODUCE.md + metrics.csv + notes.md
4. **çŸ­ä½œä¸š + æ¬¡æ—¥éªŒæ”¶**ï¼šæ¯å¤©2å°æ—¶ç”¨äºå‡†å¤‡ã€æäº¤ã€è¯Šæ–­ã€å†™ç»“è®º

## âš ï¸ æ³¨æ„äº‹é¡¹
=======
Day 3 ä¼šé¢å¤–å†™å…¥ `results/week01/metrics.csv`ï¼ŒåŒ…å«æ¯ä¸ª epoch çš„ `train_loss` ä¸ `val_acc`ã€‚

## æ³¨æ„äº‹é¡¹
>>>>>>> Stashed changes

1. é¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ MNIST æ•°æ®é›†
2. å¦‚æœä½¿ç”¨ GPUï¼Œç¡®ä¿å·²æ­£ç¡®å®‰è£… CUDA å’Œ cuDNN
3. `device: "auto"` ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å¯ç”¨çš„ GPUï¼Œå¦åˆ™ä½¿ç”¨ CPU
4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨æ•°æ®é›†å’Œç»“æœ
5. å¤šå¡è®­ç»ƒéœ€è¦ç¡®ä¿ NCCL ç¯å¢ƒæ­£ç¡®é…ç½®

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [Plan.md](Plan.md) - è¯¦ç»†çš„å­¦ä¹ è®¡åˆ’
- [REPRODUCE.md](my-dl-plan/REPRODUCE.md) - å¤ç°æŒ‡å—ï¼ˆå¾…å®Œå–„ï¼‰
- [notes/env.md](my-dl-plan/notes/env.md) - ç¯å¢ƒé…ç½®ç¬”è®°

## ğŸ“… æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|------|----------|
| 2025-12-20 | Day 1: åˆ›å»º repoï¼Œç¯å¢ƒé…ç½®ï¼ŒåŸºç¡€è®­ç»ƒè„šæœ¬ |
| 2025-12-21 | Day 2: MNIST æ•°æ®åŠ è½½å™¨å®ç° |

## ğŸ“„ è®¸å¯è¯

[æ ¹æ®é¡¹ç›®å®é™…æƒ…å†µæ·»åŠ è®¸å¯è¯ä¿¡æ¯]
